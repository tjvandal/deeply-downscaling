!obj:pylearn2.train.Train {
      dataset: &train !obj:pylearn2.datasets.transformer_dataset.TransformerDataset {
            raw: !obj:load_data.load_supervised {
                batchsize: 50
            },
            transformer: !pkl: "./grbm_l1.pkl"
        },
    model: !obj:pylearn2.models.mlp.MLP {
        batch_size: 50,
        layers: [
                 !obj:pylearn2.models.mlp.PretrainedLayer {
                     layer_name: 'h1',
                     layer_content: !pkl: "grbm_l1.pkl"
                 },
                 !obj:pylearn2.models.mlp.PretrainedLayer {
                     layer_name: 'h2',
                     layer_content: !pkl: "dbn_l2.pkl"
                 },
                 !obj:pylearn2.models.mlp.Softmax {
                     max_col_norm: 1.9365,
                     layer_name: 'y',
                     n_classes: 10,
                     irange: .005
                 }
                ],
        nvis: 325
    },
       algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
               # We initialize the learning rate and momentum here. Down below
               # we can control the way they decay with various callbacks.
               learning_rate: 1e-3,
               # Compute new model parameters using SGD + Momentum
               learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
                   init_momentum: 0.5,
               },
               # These arguments say to compute the monitoring channels on 10 batches
               # of the training set.
               monitoring_batches: 10,
               monitoring_dataset : {'train': *train},
                cost: !obj:pylearn2.costs.mlp.Default {},
               # We tell the RBM to train for 300 epochs
               termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter { max_epochs: 15 },
               update_callbacks: [
                # This callback makes the learning rate shrink by dividing it by decay_factor after
                # each sgd step.
                !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
                        decay_factor: 1.000015,
                        min_lr:       0.0001
                }
           ]
        },
    extensions: [
            # This callback makes the momentum grow to 0.9 linearly. It starts
            # growing at epoch 5 and finishes growing at epoch 6.
            !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
                final_momentum: .9,
                start: 5,
                saturate: 6
            },
    ],
    save_path: "./mlp_svm.pkl",
    save_freq: 1
}
