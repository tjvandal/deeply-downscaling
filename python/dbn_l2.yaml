!obj:pylearn2.train.Train {
      dataset: &train !obj:pylearn2.datasets.transformer_dataset.TransformerDataset {
            raw: !obj:load_data.load_pretraining {
                batchsize: 50
            },
            transformer: !pkl: "./grbm_l1.pkl"
        },
    model: !obj:pylearn2.models.dbm.DBM {
        batch_size: 50,
        # 1 mean field iteration reaches convergence in the RBM
        niter: 2,
        # The visible layer of this RBM is just a binary vector
        # (as opposed to a binary image for convolutional models,
        # a Gaussian distributed vector, etc.)
        visible_layer: !obj:pylearn2.models.dbm.BinaryVector {
            nvis: 300,
        },
        hidden_layers: [
            # This RBM has one hidden layer, consisting of a binary vector.
            # Optionally, one can do max pooling on top of this vector, but
            # here we don't, by setting pool_size = 1.
            !obj:pylearn2.models.dbm.BinaryVectorMaxPool {
                layer_name: 'h1',
                detector_layer_dim: 150,
                pool_size: 1,
                irange: .05,
                init_bias: -2.,
            },
            !obj:pylearn2.models.dbm.BinaryVectorMaxPool {
                layer_name: 'h',
                detector_layer_dim: 50,
                pool_size: 1,
                irange: .05,
                init_bias: -2.,
            }
       ]
    },
       algorithm: !obj:pylearn2.training_algorithms.sgd.SGD {
               # We initialize the learning rate and momentum here. Down below
               # we can control the way they decay with various callbacks.
               learning_rate: 1e-3,
               # Compute new model parameters using SGD + Momentum
               learning_rule: !obj:pylearn2.training_algorithms.learning_rule.Momentum {
                   init_momentum: 0.5,
               },
               # These arguments say to compute the monitoring channels on 10 batches
               # of the training set.
               monitoring_batches: 10,
               monitoring_dataset : *train,
                cost : !obj:pylearn2.costs.autoencoder.MeanSquaredReconstructionError {},
               # We tell the RBM to train for 300 epochs
               termination_criterion: !obj:pylearn2.termination_criteria.EpochCounter { max_epochs: 15 },
               update_callbacks: [
                # This callback makes the learning rate shrink by dividing it by decay_factor after
                # each sgd step.
                !obj:pylearn2.training_algorithms.sgd.ExponentialDecay {
                        decay_factor: 1.000015,
                        min_lr:       0.0001
                }
           ]
        },
    extensions: [
            # This callback makes the momentum grow to 0.9 linearly. It starts
            # growing at epoch 5 and finishes growing at epoch 6.
            !obj:pylearn2.training_algorithms.learning_rule.MomentumAdjustor {
                final_momentum: .9,
                start: 5,
                saturate: 6
            },
    ],
    save_path: "./dbn_l2.pkl",
    save_freq: 1
}
